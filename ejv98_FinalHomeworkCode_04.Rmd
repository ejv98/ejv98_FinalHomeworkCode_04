---
title: "ejv98_FinalHomeworkCode_04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

<!-- [1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data -->

```{r}
z.prop.test <- function(p1, n1, p0, p2= NULL, n2= NULL, alternative="two.tailed", conf.level=0.95){
  #error messages for rule of thumb
   if (n1*p0 < 5)
  {
  print("Non Normal!")
  }
if (n1*(1-p0) < 5)
  {
  print("Non Normal!")
  }
#only if using p2 and n2
if (!is.null(n2))
{
  if (n2*p0 < 5)
  {
  print("Non Normal!")
  }
  if (n2*(1-p0) < 5)
  {
  print("Non Normal!")
  }
  # this is the function for z test (no errors)
  if (alternative == "two.tailed")
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1) #z score
    p<- pnorm(z, lower.tail = TRUE) #p value
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1) #confidence intervals
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
   }
#can be used for one sample, when n2 and p2 are null
  if (is.null(n2)|is.null(p2))
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1)
    p<- pnorm(z, lower.tail = TRUE)
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
  }
#if p1 is less than p2 (two samples)
    if (alternative == "less")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion
    z<- (p2 - p1)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    lwr <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) # ci intervals
    upr <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
    ci <- c(lwr, upr)
  }
 # p1 is greater than p2 (two samples)
   if (alternative == "greater")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) 
  #this is for a pooled portion
    z<- (p1 - p2)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    lwr <- ((p1-p2)-qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1) #ci intervals
    upr <- ((p1-p2)+qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1)
    ci <- c(lwr, upr)
  }
values<- c("z-stat", z, "p value", p, "conf. intervals", ci)
print(values)
}
}
```

*The "alternative" part of the function is just indicating to the function which side of the distribution you are looking at (in a one or two sample test). Everything else in the function should be constant except for the "lower.tail" part of defining p (so basically you dont have to repeat the CI equation each time).*


<!-- [2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size): -->

```{r}
library(curl)
library(ggplot2)
#loading the packages and loading the data (below)
f <- curl('https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv')
d <- read.csv(f, stringsAsFactors = FALSE, header = TRUE)
head(d)
#preparing the data and removing "na"
c<-na.omit(d) 
#making the data fram compatible with ggplot
h<-data.frame(c) 
x<-d$MaxLongevity_m 
y<-d$Brain_Size_Species_Mean 
#identifying the variables
```

```{r}
#using lm
M1<-lm(data = d, y~x) 
M1
summary(M1)
#r2 is .4887
ggM1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
ggM1
#ggplot of the first model
```

<!-- Identify and interpret the point estimate of the slope (β1 -->
<!-- ), as well as the outcome of the test associated with the hypotheses H0: β1 -->
<!--  = 0; HA: β1 -->
<!--  ≠ 0. Also, find a 90 percent CI for the slope (β1 -->
<!-- ) parameter. -->

```{r}
#finding beta0 and beta1 
t1 <- unlist(M1$coefficients)
# unlisting is used to get coeficients out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#to find the 90% CI
ci.slope<-confint(M1, level = 0.9)
ci.slope
```

*what are your interpretations of the coefficients?*

<!-- Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines. -->

```{r}
#confidence intervals for 90%
ci <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
ci.frame<-data.frame(ci)
#finding the prediction intervals for 90%
pi <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
pi.frame<-data.frame(pi)
#combining the original x and y values
r<-cbind(x, y) 
#combining the x and y values, and the ci and pi data frames!
New<-cbind(r, ci.frame, pi.frame)
#naming the columns
names(New) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(New)
#making the ggplot
gMod2<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupper), colour = "red")
gMod2
```

<!-- [3]Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not? -->

```{r}
#point estimate
predict(M1, newdata = data.frame(x = 800))
#258.2979
#PIs
predict(M1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
#fit = 258.2979, lwr = 166.6757, upr = 349.9201
#I don't thik this model is accurate because it is meant to predict brain sizes and longevity of animals with a similar brain size. 800 grams is so much more than the brain sizes in the data set,I don't think the same relationship would exist. 
```

<!-- log model -->

```{r}
#log transform the variables and make them into a data frame 
r1<-log(d$MaxLongevity_m)
r2<-log(d$Brain_Size_Species_Mean)
df2<-as.data.frame(cbind(r2,r1))
#transformed model (below)
logM1<-lm(data = d, log(y)~log(x))
logM1
summary(logM1)
#the r2 is now 0.5751, higher than regular model
#making the ggplot
ggM2 <- ggplot(data=df2, aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")+ geom_point() + geom_smooth(method="lm", fullrange=TRUE)
ggM2
```

```{r}
#finding beta0 and beta1 
t1 <- unlist(logM1$coefficients)
# unlist to get coeficients out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```
```{r}
#CI for 90%
ci <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
logci.frame<-data.frame(ci)
#PI for 90%
pi <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
logpi.frame<-data.frame(pi)
#combine the original x and y values
r<-cbind(x, y) 
#combining the x and y values, and the ci and pi data frames
logNew<-cbind(r, ci.frame, pi.frame)
#naming the columns
names(logNew) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(logNew)
#making the ggplot
loggMod2<-ggplot(data = logNew, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = logNew, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = logNew, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = logNew, aes(x = x, y = PIupper), colour = "red")
loggMod2
```

```{r}
#point estimate
predict(logM1, newdata = data.frame(x = 800))
#258.2979
#PIs
predict(logM1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
```

*Do you trust this prediction that your model made for Brain_Size_Species_Mean = 800??*

Looking at your two models, which do you think is better? Why?
#The log model appears to works better - it gave a higher r2 value, which means a strong linear relationship.

*code looks good, but dont forget to interpret it*
